{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v12T7pxpqjQC"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg #загрузка актуальной версии whisper\n",
        "!pip install git+https://github.com/eternnoir/pyTelegramBotAPI.git\n",
        "!pip install moviepy\n",
        "!wget   http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 # DOWNLOAD LINK\n",
        "!bunzip2 /content/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install matplotlib>=3.3.2\n",
        "BRANCH = 'r1.21.0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "# Loading weights, config and example wav for CTC-model\n",
        "!wget https://n-ws-q0bez.s3pd12.sbercloud.ru/b-ws-q0bez-jpv/GigaAM/emo_model_weights.ckpt\n",
        "!wget https://n-ws-q0bez.s3pd12.sbercloud.ru/b-ws-q0bez-jpv/GigaAM/emo_model_config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6MFShoulpfN"
      },
      "outputs": [],
      "source": [
        "token = \"token\"\n",
        "import cv2\n",
        "import numpy as np\n",
        "import dlib\n",
        "import whisper #импортируется модуль whisper\n",
        "model1=whisper.load_model('medium') #загружается размер модели small(можно указать любую модель Whisper(base,tiny,small, medium, large))\n",
        "import telebot\n",
        "import os\n",
        "from moviepy.editor import *\n",
        "from google.colab.patches import cv2_imshow\n",
        "import nltk # Импортируем библиотеку nltk\n",
        "nltk.download('stopwords') # Импортируем корпус с русскими стоп-словами\n",
        "nltk.download('punkt') # Загружаем необходимые данные punkt, которые используются для токенизации текста.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "datFile =  \"/content/shape_predictor_68_face_landmarks.dat\"\n",
        "from typing import List, Union\n",
        "from omegaconf import OmegaConf\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from omegaconf import DictConfig, ListConfig\n",
        "import hydra\n",
        "\n",
        "class SpecScaler(torch.nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.log(x.clamp_(1e-9, 1e9))\n",
        "\n",
        "\n",
        "class GigaAMEmo(torch.nn.Module):\n",
        "    def __init__(self, conf: Union[DictConfig, ListConfig]):\n",
        "        super().__init__()\n",
        "        self.id2name = conf.id2name\n",
        "        self.feature_extractor = hydra.utils.instantiate(conf.feature_extractor)\n",
        "        self.conformer = hydra.utils.instantiate(conf.encoder)\n",
        "        self.linear_head = hydra.utils.instantiate(conf.classification_head)\n",
        "\n",
        "    def forward(self, features, features_length=None):\n",
        "        if features.dim() == 2:\n",
        "            features = features.unsqueeze(0)\n",
        "        if not features_length:\n",
        "            features_length = torch.ones(features.shape[0]) * features.shape[-1]\n",
        "            features_length = features_length.to(features.device)\n",
        "        encoded, _ = self.conformer(audio_signal=features, length=features_length)\n",
        "        encoded_pooled = torch.nn.functional.avg_pool1d(\n",
        "            encoded, kernel_size=encoded.shape[-1]\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        logits = self.linear_head(encoded_pooled)\n",
        "        return logits\n",
        "\n",
        "    def get_probs(self, audio_path: str) -> List[List[float]]:\n",
        "        audio_signal, _ = sf.read(audio_path, dtype=\"float32\")\n",
        "        features = self.feature_extractor(torch.tensor(audio_signal).float().to(next(self.parameters()).device))\n",
        "        logits = self.forward(features)\n",
        "        probs = torch.nn.functional.softmax(logits).detach().tolist()\n",
        "        return probs\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "from omegaconf import OmegaConf\n",
        "import torchaudio\n",
        "from pydub import AudioSegment\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ry7PQHuBwh_7"
      },
      "outputs": [],
      "source": [
        "bot = telebot.TeleBot(token)\n",
        "@bot.message_handler(commands=['start', 'help'])\n",
        "def send_welcome(message):\n",
        "\tbot.reply_to(message, \" Приветствую! Это бот для анализа выступления спикера.\"\n",
        "\t+\" \\n Пришлие видео и я пришлю его анализ.\"\n",
        "\t+\" \\n Анализ включает в себя:\"\n",
        "\t    +\" \\n - текст, без слов паразитов\"\n",
        "\t    +\" \\n - эмоцию которую транслирует спикер через текст\"\n",
        "\t    +\" \\n - количество излишне больших наклонов головы\"\n",
        "\t    +\" \\n - количество отвода глаз вправо и влево\"\n",
        "\t    +\" \\n - сколько раз поджал губы\"\n",
        "\t    +\" \\n По команде: /help или /start будет отправлено это сообщение.\")\n",
        "############################## video-analys\n",
        "@bot.message_handler(content_types=['video'])\n",
        "def send_text(message):\n",
        "  bot.reply_to(message, 'Получил')\n",
        "  file_info = bot.get_file(message.video.file_id)\n",
        "################################ check folders\n",
        "  dirs = set(os.listdir())\n",
        "  if \"audio\" not in dirs:\n",
        "    os.mkdir(\"/content/audio\")\n",
        "    dirs = set(os.listdir())\n",
        "  if \"videos\" not in dirs:\n",
        "    os.mkdir(\"/content/videos\")\n",
        "    dirs = set(os.listdir())\n",
        "################################ download\n",
        "  bot.reply_to(message, \"Скачиваю\")\n",
        "  downloaded_file = bot.download_file(file_info.file_path)\n",
        "  src =  file_info.file_path\n",
        "  with open(src, 'wb') as new_file:\n",
        "      new_file.write(downloaded_file)\n",
        "################################ recive vido+audio\n",
        "  bot.reply_to(message, 'Извлекаю аудио и видео')\n",
        "  video = VideoFileClip(src)\n",
        "  video.audio.write_audiofile(\"audio.mp3\")\n",
        "  sound = AudioSegment.from_mp3('/content/audio.mp3')\n",
        "  sound = sound.set_channels(1)\n",
        "  sound.export(\"/content/audio/audio.mp3\", format=\"mp3\")\n",
        "################################ work with audio\n",
        "  for elem in os.listdir(\"/content/audio\"):\n",
        "    if elem ==\".ipynb_checkpoints\":\n",
        "      pass\n",
        "    else:\n",
        "      bot.reply_to(message, \"Анализирую аудио\")\n",
        "      src = \"/content/audio/\"+str(elem)\n",
        "      with open(src,'rb') as f:\n",
        "        #print(src)\n",
        "        ################ recive text\n",
        "        result=model1.transcribe(src,fp16=False) #транскрибация файла. Файл необходимо загрузить в colab,раздел \"файлы\". Параметр fp16=False указывает на использование 32-битного числового формата при транскрибировании.\n",
        "        result_text=result['text'] # Результат работы модели сохраняется в переменную result\n",
        "        ################ filter text\n",
        "        result_text = result_text.lower() #Перевод в нижний регистр\n",
        "        result_text2 = result_text.replace('.', '').replace(',', '').replace('–', '').replace('!', '').replace('?', '') #Чистим текст от знаков пунктуации\n",
        "        result_text3 = result_text2.replace(\"wasn't\", \"was not\").replace(\"'m\", \"am\").replace(\"'re\", \"are\").replace(\"'s\", \"is\").replace(\"'d\", \"would\").replace(\"'d\", \"had\").replace(\"'d\", \"did\").replace(\"'ll\", \"will\").replace(\"'ve\", \"have\").replace(\"n't\", \"not\").replace(\"can't\", \"cannot\").replace(\"won't\", \"will not\").replace(\"shan't\", \"shall not\").replace(\"wouldn't\", \"would not\").replace(\"couldn't\", \"could not\").replace(\"shouldn't\", \"should not\").replace(\"mustn't\", \"must not\").replace(\"hasn't\", \"has not\").replace(\"haven't\", \"have not\").replace(\"hadn't\", \"had not\").replace(\"isn't\", \"is not\").replace(\"aren't\", \"are not\").replace(\"weren't\", \"were not\").replace(\"don't\", \"do not\").replace(\"doesn't\", \"does not\").replace(\"didn't\", \"did not\").replace(\"ain't\", \"am not\").replace(\"ain't\", \"is not\").replace(\"ain't\", \"are not\").replace(\"let's\", \"let us\").replace(\"needs't\", \"needs not\").replace(\"use't\", \"used not\").replace(\"dare't\", \"dare not\").replace(\"may'd\", \"may would\").replace(\"may'd\", \"may had\").replace(\"might'd\", \"might would\").replace(\"might'd\", \"might had\").replace(\"must'd\", \"must would\").replace(\"must'd\", \"must had\").replace(\"shall'd\", \"shall would\").replace(\"shall'd\", \"shall had\").replace(\"should'd\", \"should would\").replace(\"should'd\", \"should had\").replace(\"will'd\", \"will would\").replace(\"will'd\", \"will had\").replace(\"would'd\", \"would would\").replace(\"would'd\", \"would had\")\n",
        "        tokens = word_tokenize(result_text3) # Разбиваем текст на слова\n",
        "        stop_words = stopwords.words('russian') # Русские слова-паразиты\n",
        "        stop_words += stopwords.words('english') # Английские слова-паразиты\n",
        "        newStopWords = ['короче', 'алло', 'таки', 'там', 'вот', 'так', 'уже', 'такой', 'такая', 'такие', 'как', 'бы', 'то', 'есть', 'эта', 'ну', 'блин', 'вообще', 'вон', 'это']\n",
        "        stop_words.extend(newStopWords) #Расширяем список русских слов-паразитов\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words] # Очищаем текст от слов-паразитов\n",
        "        filtered_audio = \" \".join(filtered_tokens)\n",
        "        bot.reply_to(message, \"Отфильтрованный текст от слов-паразитов: \"+filtered_audio)\n",
        "        ################ recive text emotion\n",
        "        model_config = 'emo_model_config.yaml'\n",
        "        model_weights = 'emo_model_weights.ckpt'\n",
        "        audio_path = src[:]\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        conf = OmegaConf.load(model_config)\n",
        "        model = GigaAMEmo(conf)\n",
        "        ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "        model.load_state_dict(ckpt, strict=False)\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            probs = model.get_probs(audio_path)[0]\n",
        "        emotion = max(enumerate(probs), key=lambda item: item[1])\n",
        "        bot.reply_to(message, \"Эмоция выступления: \"+str(model.id2name[emotion[0]]))\n",
        "      ################clear\n",
        "      os.remove(src)\n",
        "################################ work with video\n",
        "  for elem in os.listdir(\"/content/videos\"):\n",
        "    if elem ==\".ipynb_checkpoints\":\n",
        "      pass\n",
        "    else:\n",
        "      src = \"/content/videos/\"+str(elem)\n",
        "      with open(src,'rb') as f:\n",
        "        ################ count head tilt\n",
        "        capture = cv2.VideoCapture(filename = src)\n",
        "        angle = 0\n",
        "        head_tilt = False\n",
        "        head_tilt_count = 0\n",
        "        while True:\n",
        "            ret, frame = capture.read()\n",
        "            if frame is None:\n",
        "              break\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
        "            x, y, w, h = 0, 0, 0, 0\n",
        "            for (x, y, w, h) in faces:\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            eyes = eye_cascade.detectMultiScale(gray[y:(y + h), x:(x + w)], 1.1, 4)\n",
        "            index = 0\n",
        "            eye_1 = [None, None, None, None]\n",
        "            eye_2 = [None, None, None, None]\n",
        "            for (ex, ey, ew, eh) in eyes:\n",
        "              if index == 0:\n",
        "                  eye_1 = [ex, ey, ew, eh]\n",
        "              elif index == 1:\n",
        "                  eye_2 = [ex, ey, ew, eh]\n",
        "              cv2.rectangle(frame[y:(y + h), x:(x + w)], (ex, ey),\n",
        "                (ex + ew, ey + eh), (0, 0, 255), 2)\n",
        "              index = index + 1\n",
        "            if (eye_1[0] is not None) and (eye_2[0] is not None):\n",
        "              if eye_1[0] < eye_2[0]:\n",
        "                  left_eye = eye_1\n",
        "                  right_eye = eye_2\n",
        "              else:\n",
        "                  left_eye = eye_2\n",
        "                  right_eye = eye_1\n",
        "              left_eye_center = (\n",
        "              int(left_eye[0] + (left_eye[2] / 2)),\n",
        "              int(left_eye[1] + (left_eye[3] / 2)))\n",
        "              right_eye_center = (\n",
        "              int(right_eye[0] + (right_eye[2] / 2)),\n",
        "              int(right_eye[1] + (right_eye[3] / 2)))\n",
        "              left_eye_x = left_eye_center[0]\n",
        "              left_eye_y = left_eye_center[1]\n",
        "              right_eye_x = right_eye_center[0]\n",
        "              right_eye_y = right_eye_center[1]\n",
        "              delta_x = right_eye_x - left_eye_x\n",
        "              delta_y = right_eye_y - left_eye_y\n",
        "              if delta_x != 0:\n",
        "                angle = np.arctan(delta_y / delta_x)\n",
        "              else:\n",
        "                angle = 0\n",
        "            angle = (angle * 180) / np.pi\n",
        "            if angle > 30:\n",
        "                cv2.putText(frame, 'RIGHT TILT :' + str(int(angle))+' degrees',\n",
        "                            (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                            (0, 0, 0), 2, cv2.LINE_4)\n",
        "                if not head_tilt:\n",
        "                    head_tilt = True\n",
        "                    head_tilt_count += 1\n",
        "            elif angle < -30:\n",
        "                cv2.putText(frame, 'LEFT TILT :' + str(int(angle))+' degrees',\n",
        "                            (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                            (0, 0, 0), 2, cv2.LINE_4)\n",
        "                if not head_tilt:\n",
        "                    head_tilt = True\n",
        "                    head_tilt_count += 1\n",
        "            else:\n",
        "                cv2.putText(frame, 'STRAIGHT :', (20, 30),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                            (0, 0, 0), 2, cv2.LINE_4)\n",
        "                head_tilt = False\n",
        "            cv2_imshow(frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        msg1='Number of head tilts: '+ str(head_tilt_count)\n",
        "        capture.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        bot.reply_to(message, msg1)\n",
        "      ################ count eyes moving\n",
        "      capture = cv2.VideoCapture(filename = src)\n",
        "      previous_position = None\n",
        "      left_eye_counter = 0\n",
        "      right_eye_counter = 0\n",
        "      while True:\n",
        "          ret, frame = capture.read()\n",
        "          if frame is None:\n",
        "            break\n",
        "          gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "          eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "          current_position = None\n",
        "          for (ex, ey, ew, eh) in eyes:\n",
        "              cv2.rectangle(frame, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2)\n",
        "              eye_center_x = ex + ew // 2\n",
        "              if eye_center_x < frame.shape[1] // 2:\n",
        "                  current_position = 'LEFT'\n",
        "              elif eye_center_x > frame.shape[1] * 2 // 3:\n",
        "                  current_position = 'RIGHT'\n",
        "          if current_position != previous_position:\n",
        "              if current_position == 'LEFT':\n",
        "                  cv2.putText(frame, 'LEFT', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "                  left_eye_counter +=1\n",
        "              elif current_position == 'RIGHT':\n",
        "                  cv2.putText(frame, 'RIGHT', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "                  right_eye_counter +=1\n",
        "              previous_position = current_position\n",
        "          cv2_imshow(frame)\n",
        "          if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "              break\n",
        "      msg2='Number of eye motions to right: ' +str(left_eye_counter) + ' To left: ' +str(right_eye_counter)\n",
        "      bot.reply_to(message, msg2)\n",
        "      capture.release()\n",
        "      cv2.destroyAllWindows()\n",
        "      ################ count lip presses\n",
        "      detector = dlib.get_frontal_face_detector()\n",
        "      predictor = dlib.shape_predictor(datFile)\n",
        "      capture = cv2.VideoCapture(filename = src)\n",
        "      lip_press_count = 0\n",
        "      prev_lips_pressed = False\n",
        "      while True:\n",
        "        ret, frame = capture.read()\n",
        "        if frame is None:\n",
        "          print(\"Error: Failed to capture frame from camera.1\")\n",
        "          break\n",
        "        if not ret:\n",
        "          print(\"Error: Failed to capture frame from camera.2\")\n",
        "          break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray)\n",
        "\n",
        "        for face in faces:\n",
        "          landmarks = predictor(gray, face)\n",
        "          mouth_points = []\n",
        "          for n in range(48, 68):\n",
        "              x = landmarks.part(n).x\n",
        "              y = landmarks.part(n).y\n",
        "              mouth_points.append((x, y))\n",
        "\n",
        "          # Calculate the distance between upper and lower lip\n",
        "          lip_distance = mouth_points[15][1] - mouth_points[16][1]\n",
        "\n",
        "          # If the distance is less than a threshold and lips were not previously pressed, increment the counter\n",
        "          if lip_distance < 1 and not prev_lips_pressed:\n",
        "              lip_press_count += 1\n",
        "              prev_lips_pressed = True\n",
        "          elif lip_distance >= 1:\n",
        "              prev_lips_pressed = False\n",
        "          x, y, w, h = cv2.boundingRect(np.array(mouth_points))\n",
        "          cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        cv2_imshow(frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "          break\n",
        "      msg3=\"Total lip presses: \" + str(lip_press_count)\n",
        "      bot.reply_to(message, msg3)\n",
        "      ################ clear\n",
        "      os.remove(src)\n",
        "################################\n",
        "bot.infinity_polling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjNKlHUsylGc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}